{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQm7Br285MrIMa5cRlFM+Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isa-ulisboa/greends-pml/blob/main/iris_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqcKbIWz75HC",
        "outputId": "8f360c17-9aa4-461e-f820-9d89efcb1ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'learning_rate': 0.001, 'n_iterations': 50}\n",
            "Accuracy on test set: 0.4\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=100):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.weights = np.zeros(1 + X.shape[1])\n",
        "        self.errors = []\n",
        "\n",
        "        for _ in range(self.n_iterations):\n",
        "            errors = 0\n",
        "            for xi, target in zip(X, y):\n",
        "                update = self.learning_rate * (target - self.predict(xi))\n",
        "                self.weights[1:] += update * xi\n",
        "                self.weights[0] += update\n",
        "                errors += int(update != 0.0)\n",
        "            self.errors.append(errors)\n",
        "        return self\n",
        "\n",
        "    def net_input(self, X):\n",
        "        return np.dot(X, self.weights[1:]) + self.weights[0]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.where(self.net_input(X) >= 0.0, 1, -1)\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data[:100, :4]  #\n",
        "y = iris.target[:100]\n",
        "\n",
        "# Split data into training, development, and testing sets\n",
        "X_train, X_devtest, y_train, y_devtest = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_dev, X_test, y_dev, y_test = train_test_split(X_devtest, y_devtest, test_size=0.5, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "X_dev_std = scaler.transform(X_dev)\n",
        "X_test_std = scaler.transform(X_test)\n",
        "\n",
        "# Initialize perceptron with default parameters\n",
        "perceptron = Perceptron()\n",
        "\n",
        "# Hyperparameters to tune\n",
        "learning_rates = [0.001, 0.01]\n",
        "n_iterations = [50, 100, 150]\n",
        "\n",
        "best_accuracy = 0\n",
        "best_hyperparams = {}\n",
        "\n",
        "# Grid search over hyperparameters using development set\n",
        "for lr in learning_rates:\n",
        "    for n_iter in n_iterations:\n",
        "        perceptron.learning_rate = lr\n",
        "        perceptron.n_iterations = n_iter\n",
        "\n",
        "        perceptron.fit(X_train_std, y_train)\n",
        "        y_dev_pred = perceptron.predict(X_dev_std)\n",
        "        accuracy = accuracy_score(y_dev, y_dev_pred)\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_hyperparams = {'learning_rate': lr, 'n_iterations': n_iter}\n",
        "\n",
        "# Train with the best hyperparameters on combined training and development set\n",
        "perceptron.learning_rate = best_hyperparams['learning_rate']\n",
        "perceptron.n_iterations = best_hyperparams['n_iterations']\n",
        "X_train_dev_std = np.vstack((X_train_std, X_dev_std))\n",
        "y_train_dev = np.concatenate((y_train, y_dev))\n",
        "perceptron.fit(X_train_dev_std, y_train_dev)\n",
        "\n",
        "# Predictions on the test set\n",
        "y_pred = perceptron.predict(X_test_std)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Best hyperparameters:\", best_hyperparams)\n",
        "print(\"Accuracy on test set:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pQJF_NPxBaUb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}